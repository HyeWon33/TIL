## 7.5 부스팅

#### 부스팅 

- 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법
- 예시
  - 에이다부스트
  - 그레이디언트 부스팅



### 7.5.1 에이다부스트

- 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것

- 예를들어

  - 첫 번째 분류기를 훈련 세트에서 훈련시키고 예측을 만든다.
  - 그 다음 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높인다.
  - 두 번째 분류기는 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 다시 예측을 만든다.
  - 그다음에 다시 가중치를 업데이트 하는 식으로 계속된다.

- 경사 하강법과 비슷한 면이 있다. 경사 하강법은 비용 함수를 최소화하기 위해 한 예측기의 모델 파라미터를 조정해가는 반면 에이다부스트는 점차 더 좋아지도록 앙상블에 예측기를 추가한다.

- ```python
  m = len(X_train)
  
  fig, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)
  for subplot, learning_rate in ((0, 1), (1, 0.5)):
      sample_weights = np.ones(m) / m
      plt.sca(axes[subplot])
      for i in range(5):
          svm_clf = SVC(kernel="rbf", C=0.2, gamma=0.6, random_state=42)
          svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)
          y_pred = svm_clf.predict(X_train)
          r = sample_weights[y_pred != y_train].sum() / sample_weights.sum() # equation 7-1
          alpha = learning_rate * np.log((1 - r) / r) # equation 7-2
          sample_weights[y_pred != y_train] *= np.exp(alpha) # equation 7-3
          sample_weights /= sample_weights.sum() # normalization step
          plot_decision_boundary(svm_clf, X, y, alpha=0.2)
          plt.title("learning_rate = {}".format(learning_rate), fontsize=16)
      if subplot == 0:
          plt.text(-0.7, -0.65, "1", fontsize=14)
          plt.text(-0.6, -0.10, "2", fontsize=14)
          plt.text(-0.5,  0.10, "3", fontsize=14)
          plt.text(-0.4,  0.55, "4", fontsize=14)
          plt.text(-0.3,  0.90, "5", fontsize=14)
      else:
          plt.ylabel("")
  
  save_fig("boosting_plot")
  plt.show()
  ```

  - ![그림7-8](https://user-images.githubusercontent.com/52944554/160782357-97962795-5a15-48c9-91eb-060a2526e5b1.PNG)
    - 가중치 변화는 1이 더 크다. 0.5는 적게 변해서 모델 학습 속도가 느리다.
  
- 모든 예측기가 훈련을 마치면 배깅이나 페이스팅과 비슷한 방식으로 예측된다. 하지만 가중치가 적용된 훈련 세트의 전반적인 정확도에 따라 예측기마다 다른 가중치가 적용된다.

- ![식7-1, 7-2](https://user-images.githubusercontent.com/52944554/160782409-794c89ce-8879-4429-81f9-60754920c248.PNG)

- ![식7-3, 7-4](https://user-images.githubusercontent.com/52944554/160782478-e3bc745b-ec7b-4b24-be6f-57917164327a.PNG)

- AdaBoostClassifier를 사용하여 200개의 아주 얕은 결정 트리를 기반으로 하는 에이다부스트  분류기 훈련

  - ```python
    from sklearn.ensemble import AdaBoostClassifier
    
    ada_clf = AdaBoostClassifier(
        DecisionTreeClassifier(max_depth=1), n_estimators=200,
        algorithm="SAMME.R", learning_rate=0.5, random_state=42)
    ada_clf.fit(X_train, y_train)
    ```

  - ```python
    plot_decision_boundary(ada_clf, X, y)
    ```

  - ![그림7-8 2](https://user-images.githubusercontent.com/52944554/160782651-89cf9555-7d66-4464-b84e-453e0928c40f.PNG)

  



### 7.5.2 그레이디언트 부스팅

- 에이다부스트처럼 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다. 하지만 에이다부스트처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시킨다.

- 그레이디언트 트리 부스팅 : 결정 트리를 기반 예측기로 사용하는 회귀

- ```python
  np.random.seed(42)
  X = np.random.rand(100, 1) - 0.5
  y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)
  
  
  from sklearn.tree import DecisionTreeRegressor
  
  tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg1.fit(X, y)
  
  
  y2 = y - tree_reg1.predict(X) #오차 = 타겟 - 예측값
  #y2에 대해서 다시 모델을 만들어서 훈련한다. 오차가 타겟값이 된다.
  tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg2.fit(X, y2)
  
  
  y3 = y2 - tree_reg2.predict(X)
  tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg3.fit(X, y3)
  #세 개의 트리를 포함하는 앙상블 모델 만들었다.
  
  X_new = np.array([[0.8]]) #0.8에 대한 샘플을 만든다.
  
  
  #예측할 때는 예측 다 더해서 예측값 만든다.
  y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
  
  
  y_pred 
  >>> array([0.75026781])
  ```

  - 생성 코드...
  - ![그림7-9 2](https://user-images.githubusercontent.com/52944554/160782710-ac2a1080-c931-473c-afce-fb48140e3824.jpg)
  - 1 : 위에서 랜덤하게 만든 2차원 데이터셋
  - 2 : 앙상블에 트리가 하나만 있어서 첫 번째 트리의 예측과 완전히 같다.
  - 3 : 첫 번째 트리의 잔여 오차에 대해 학습된다.
  - 4 : 앙상블 예측이 이 두 개의 트리 예측의 합과 같다.
  - 5 : 또 다른 트리가 두 번째 트리의 잔여 오차에 훈련되었다.
  - 트리가 앙상블에 추가될수록 앙상블의 예측이 점차 좋아지는 것을 알 수 있다.

- 사이킷런의 GradientBoostingRegressor

  - ```python
    from sklearn.ensemble import GradientBoostingRegressor
    
    gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
    gbrt.fit(X, y)
    
    #n_estimators : 트리 수
    #learning_rate : 트리의 기여도 조절
    ```

    - 이전에 만든 것과 같은 앙상블을 만드는 코드
    - GradientBoostingRegressor를 사용하면 GBRT 앙상블을 간단하게 훈련시킬 수 있다.

  - ```python
    gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)
    gbrt_slow.fit(X, y)
    
    
    fig, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)
    
    plt.sca(axes[0])
    plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="Ensemble predictions")
    plt.title("learning_rate={}, n_estimators={}".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)
    plt.xlabel("$x_1$", fontsize=16)
    plt.ylabel("$y$", fontsize=16, rotation=0)
    
    plt.sca(axes[1])
    plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])
    plt.title("learning_rate={}, n_estimators={}".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)
    plt.xlabel("$x_1$", fontsize=16)
    
    save_fig("gbrt_learning_rate_plot")
    plt.show()
    ```

  - ![그림7-10](https://user-images.githubusercontent.com/52944554/160782766-2c4913b4-748e-488f-b42d-709df18bc4b9.PNG)

    - learning_rate를 0.1처럼 낮게 설정하면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 일반적으로 예측의 성능은 좋아진다 -> 축소라고 부르는 규제 방법.
    - 왼쪽은 훈련 세트를 학습하기에는 트리가 충분하지 않은 반면 오른쪽은 트리가 너무 많아 훈련 세트에 과대적합되어있다.



#### 조기 종료 기법

- 최적의 트리 수를 찾기 위해서

- ```python
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import mean_squared_error
  
  X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)
  
  gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)
  gbrt.fit(X_train, y_train)
  
  errors = [mean_squared_error(y_val, y_pred) #타겟값 예측값 차이 -> errors
            for y_pred in gbrt.staged_predict(X_val)]
  #staged_predict() : 트리를 추가하면서 단계적으로 각각 예측값 출력해준다.
  bst_n_estimators = np.argmin(errors) + 1
  #np.argmin(errors) : errors값 가장 작은 곳의 인덱스 값 뽑아서 인덱스는 0부터니까 1 더해서 최적의 트리 개수를 찾는다.
  
  gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)
  gbrt_best.fit(X_train, y_train)
  >>> n_estimators=56 #최적의 트리 
  
  min_error = np.min(errors) #최소 에러
  
  
  plt.figure(figsize=(10, 4)) 
  
  plt.subplot(121)
  plt.plot(np.arange(1, len(errors) + 1), errors, "b.-")
  plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], "k--")
  plt.plot([0, 120], [min_error, min_error], "k--")
  plt.plot(bst_n_estimators, min_error, "ko")
  plt.text(bst_n_estimators, min_error*1.2, "Minimum", ha="center", fontsize=14)
  plt.axis([0, 120, 0, 0.01])
  plt.xlabel("Number of trees")
  plt.ylabel("Error", fontsize=16)
  plt.title("Validation error", fontsize=14)
  
  plt.subplot(122)
  plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])
  plt.title("Best model (%d trees)" % bst_n_estimators, fontsize=14)
  plt.ylabel("$y$", fontsize=16, rotation=0)
  plt.xlabel("$x_1$", fontsize=16)
  
  save_fig("early_stopping_gbrt_plot")
  plt.show()
  ```

- ![그림7-11](https://user-images.githubusercontent.com/52944554/160782808-816fc3ca-e3f2-41d5-b03e-b1a92761d2eb.PNG)

  - 120까지 트리 값 있었는데 최소인 56에서 멈췄다. 56일 때 앙상블 트리의 예측 결과가 최적 모델이다.



#### warm_start

- 실제로 훈련을 중지하는 방법
- warm_start=True로 설정하면 사이킷런이 fit() 메서드라 호출될 때 기존 트리를 유지하고 훈련을 추가할 수 있도록 해준다.



#### XGBoost

- 라이브러리
- 사이킷런에 없음

