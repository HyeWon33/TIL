# 9장 비지도 학습

- 레이블이 없다.

- 비지도 학습과 알고리즘

  - 군집

  - 이상치 탐지

  - 밀도 추정

## 9.1 군집

- 군집 clustering : 비슷한 샘플을 구별해 하나의 **클러스터**cluster 또는 비슷한 샘플의 그룹으로 할당하는 작업이다.
  - 개체를 분류하기 위한 명확한 분류기준이 존재하지 않거나 기준이 밝혀지지 않은 상태에서 주어진 데이터들의 특성을 고려해 같은 그룹(클러스터)를 정의하고, 다른 클러스터의 개체보다 서로 더 유사한 개체가 되도록 그룹화하여 그룹의 대표성을 찾아내는 방법이다.
- 클러스터 cluster : 비슷한 특성을 가진 데이터들의 집합이다.

![그림9-1](https://user-images.githubusercontent.com/52944554/160783181-c43d5d76-2ca6-4837-a039-e85b63927aca.PNG)

- 군집 사용 애플리케이션

  - 고객 분류

  - 데이터 분석

  - 차원축소 기법

  - 이상치 탐지

  - 준지도 학습

  - 검색 엔진

  - 이미지 분할

- 클러스터에 대한 보편적인 정의는 없다. 상황에 따라 다르다.

  - 알고리즘이 다르면 다른 종류의 클러스터를 감지한다.
  - 어떤 알고리즘은
    - 센트로이드라 부르는 특정 포인트를 중심으로 모인 샘플을 찾는다.
    - 샘플이 밀집되어 연속된 영역을 찾는다. 이런 클러스터는 어떤 모양이든 될 수 있다.
    - 계층적으로 클러스터의 클러스터를 찾는다. 종류는 아주 많다.



### 9.1.1 k-평균

- k-평균 : 반복 몇 번으로 레이블이 없는 샘플 덩어리를 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘이다.

- ![그림9-2](https://user-images.githubusercontent.com/52944554/160783241-f3e968aa-dfa4-41d7-8bae-c5fc195f6480.PNG)

  - 이 데이터셋에 K-평균 군집 알고리즘을 훈련을 해본다. 이 알고리즘은 클러스터 중심을 찾고 각 샘플을 가까운 클러스터에 할당한다.

  - ```python
    from sklearn.cluster import KMeans
    k = 5
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_pred = kmeans.fit_predict(X)
    ```

  - 알고리즘이 찾을 클러스터 개수 k를 지정해야 한다. (이는 일반적으로 쉬운 일이 아니다.)

  - 각 샘플은 5개의 클러스터 중 하나에 할당된다.

  - ```python
    y_pred
    >>array([4, 0, 1, ..., 2, 1, 0], dtype=int32)
    y_pred is kmeans.labels_
    >>True
    ```

  - 이 알고리즘이 찾은 5개의 센트로이드(즉 클러스터 중심)도 확인할 수 있다.

  - ```python
    kmeans.cluster_centers_
    >>array([[-2.80389616,  1.80117999],
           [ 0.20876306,  2.25551336],
           [-2.79290307,  2.79641063],
           [-1.46679593,  2.28585348],
           [-2.80037642,  1.30082566]])
    ```

  -  새로운 샘플의 레이블을 예측할 수 있다.

  - ```python
    X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
    kmeans.predict(X_new)
    >>array([1, 1, 2, 2], dtype=int32)
    ```

  -  클러스터의 결정 경계를 그려보면 보로노이 다이어그램(가장 가까운 점의 집합으로 평균을 분할하는 방법)을 얻을 수 있다.

  - ![그림9-3](https://user-images.githubusercontent.com/52944554/160783265-95077ce1-4bbe-4c27-ba58-27a7c911dcb9.PNG)

  - 샘플은 대부분 적절한 클러스터에 잘 할당되어있다. 하지만 샘플 몇 개는 레이블이 잘못 부여되었다.

  - 샘플은 클러스터에 할당할 때 센트로이드까지 거리를 고려하는 것이 전부이기 때문이다.

  - 하드 군집 

    - 각 샘플에 가장 가까운 클러스터를 선택한다.
    - 하나의 클러스터에 할당하는 것보다 클러스터마다 샘플에 점수를 부여하는 것이 유용할 수 있다.
    - 이를 소프트 군집이라고 한다.

  - 소프트 군집

    - 각 클러스터의 거리 계산 방법이다.
    - 이 점수는 샘플과 센트로이드 사이의 거리가 될 수 있다. 반대로 가우시안 방사기저 함수와 같은 유사도 점수가 될 수 있다.

  - KMeans 클래스의 transform() 메서드는 샘플과 각 센트로이드 사이의 거리를 반환한다.

  - ```python
    kmeans.transform(X_new)
    >>array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],
           [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],
           [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],
           [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])
    ```

  - X_new에 있는 첫 번째 샘플이 첫 번째 센트로이드에서 2.81, 두 번째 센트로이드에서 0.33 ... 거리만큼 떨어져 있다.

  - 이런 고차원 데이터셋을 이런 방식으로 변환하면 k-차원 데이터셋이 만들어진다. -> 매우 효율적인 비선형 차원 축소 기법이 될 수 있다.



#### k-평균 알고리즘

- 가장 빠르고 간단한 군집 알고리즘 중 하나이다.
- 먼저 k 개의 센트로이드를 랜덤하게 초기화합니다: 데이터셋에서 k 개의 샘플을 랜덤하게 선택하고 센트로이드를 그 위치에 놓습니다.
- 수렴할 때까지 다음을 반복합니다(즉, 센트로이드가 더이상 이동하지 않을 때까지)
  - 각 샘플을 가장 가까운 센트로이드에 할당합니다.
  - 센트로이드에 할당된 샘플의 평균으로 센트로이드를 업데이트합니다.
- ![그림9-4](https://user-images.githubusercontent.com/52944554/160783290-bd6a042e-bc00-47f0-89f3-e70249ded1b1.PNG)
  - 반복 세 번 만에 일 알고리즘은 최적으로 보이는 클러스터에 도달했다.
  - 이 알고리즘이 수렴하는 것이 보장되지만 적절한 솔루션으로 수렴하지 못할 수 있다.
  - 이 여부는 센트로이드 초기화에 달려있다.
  - 랜덤한 초기화 단계에 운이 없을 때 알고리즘이 수렴할 수 있는 최적이 아닌 솔루션의 예이다.
    - ![그림9-5](https://user-images.githubusercontent.com/52944554/160783517-c88314c4-3c48-48ba-85a2-334e621bf75d.PNG)
  - 센트로이드 초기화를 개선하여 이런 위험을 줄일 수 있는 방법을 알아보자.



#### 센트로이드 초기화 방법

- 센트로이드 위치를 근사하게 알 수 있다면 (예를 들어 또 다른 군집 알고리즘을 먼저 실행한다.) init 매개변수에 센트로이드 리스트를 담은 넘파이 배열을 지정하고 n_init를 1로 설정할 수 있다.

- ```python
  good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
  kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
  ```

- 또 다른 방법은 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택하는 것이다. 랜덤 초기화 횟수는 n_init 매개변수로 조절한다. 기본값은 10이다.

- 이는 fit() 메서드를 호출할 때 앞서 전체 알고리즘이 10번 실행된다.

- 사이킷런은 이 중에 최선의 솔루션을 반환한다.

- 최선의 솔루션 -> 사용하는 성능 지표에 있다.

- 이 값은 각 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리이며 모델의 **이너셔**(거리의 제곱 합)라고 부른다.

- 위 그림의 왼쪽 모델의 이너셔 : 대략 223.3 오른쪽 모델의 이너셔 : 237.5 위 보로노이 다이어그램  모델의 이너셔 : 211.6

- KMeans 클래스는 알고리즘을 n_init번 실행하여 이너셔가 가장 낮은 모델을 반환한다.

- 이너셔 확인

  - ```python
    kmeans.inertia_
    >>211.5985372581684
    ```

  - score() 메서드는 이너셔의 음숫값을 반환한다.

  - -> 사이킷런의 큰 값이 좋은 값이다라는 규칙을 따라야 하기 때문이다. 한 예측기가 다른 것 보다 좋으면 socre() 메서드가 더  높은 값을 반환해야 한다.

  - ```python
    kmeans.score(X)
    >>-211.5985372581684
    ```

- k-평균 알고리즘을 향상시킨 k-평균++ 알고리즘

  - 다른 센트로이드와 거리가 먼 센트로이드를 선택하는 똑똑한 초기화 단계를 소개했다.
  - 이 방법은 k-평균 알고리즘이 최적이 아닌 솔루션으로 수렴할 가능성을 크게 낮춘다.
  - 반복 횟수를 크게 줄일 수 있다.
  - KMeans 클래스는 기본적으로 이 초기화 방법을 사용한다.



#### k-평균 속도 개선과 미니배치 k-평균

- 2013년 찰스 에칸 

  - k-평균 알고리즘에 대해 또 다른 중요한 개선을 제안했다.
  - 불필요한 거리 계산을 많이 피함으로써 알고리즘의 속도를 상당히 높일 수 있다.
  - 이 알고리즘은 KMeans 클래스를 기본으로 사용한다.

- 2010년 데이비드 스컬리

  - k-평균 알고리즘의 또 다른 중요한 변종이 제시되었다.

  - 전체 데이터셋을 사용해 반복하지 않고 이 알고리즘은 각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동한다. 

    - 배치 : 입력한 데이터의 묶음
    - 미니배치 : 가지고 있는 데이터가 많을 때, 모든 데이터를 다 쓰는 것이 아니고, 데이터 일부를 무작위로 추려서 그 근사치로 이용할 수 있다. 이 일부가 되는 데이터를 미니배치라고 한다.

  - 이는 일반적으로 알고리즘의 속도를 3배에서 4배 정도 높인다.

  - 메모리에 들어가지 않는 대량의 데이터셋에 군집 알고리즘을 적용할 수 있다.

  - 사이킷런은 MiniBatchKMeans 클래스에 이 알고리즘을 구현했다. KMeans 클래스처럼 사용할 수 있다.

  - ```python
    from sklearn.cluster import MiniBatchKMeans
    minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)
    minibatch_kmeans.fit(X)
    ```

  - 데이터셋이 메모리에 들어가지 않으면 가장 간단한 방법은 8장 점진적 PCA에서 했던 것처럼 memmap 클래스를 사용하는 것이다.

  - 또한 MiniBatchKMeans 클래스의 partial_fit() 메서드에서 한 번에 하나의 미니배치를 전달할 수 있다.

  - 하지만 여러 번 수행하고 만들어진 결과에서 가장 좋은 것을 직접 골라야 해서 해야 할 일이 많다.

  - 미니배치 k-평균 알고리즘이 일반 k-평균 알고리즘보다 훨씬 빠르지만 이너셔는 일반적으로 조금 더 나쁘다. 특히 클러스터의 개수가 증가살 때 그렇다.

  - ![그림9-6](https://user-images.githubusercontent.com/52944554/160783552-7ba90861-fe00-48d5-87d7-dba6e68736ad.PNG)

    - 미니배치 k-평균의 이너셔가 k-평균보다 높다. 하지만 훨씬 빠르고 k가 증가할수록 더 그렇다.



#### 최적을 클러스터 개수 찾기

- 지금까지 클러스터 개수 k를 5로 지정했다.

- 일반적으로 k를 어떻게 설정할지 쉽게 알 수 없다.

- ![그림9-7](https://user-images.githubusercontent.com/52944554/160783586-21e80eba-a194-48d6-9671-f0753e686a07.PNG)

  - 잘못된 클러스터 개수 선택 : k가 너무 작으면 별개의 클러스터를 합치고, k가 너무 크면 하나의 클러스터가 여러 개로 나뉜다.

- 가장 작은 이너셔를 가진 모델을 선택하면 되지 않을까 라고 생각할 수 있지만 그렇게 간단하지 않다.

- k=3일 때 이너셔 : 653.2 k=5일 때(211.6)보다 높다. 하지만 k=8일 때 이너셔는 119.1이다.

- 이너셔는 k가 증가함에 따라 점점 작아지므로 k를 선택할 때 좋은 성능 지표가 아니다. 실제로 클러스터가 늘어날수록 샘플은 가까운 센트로이드에 더 가깝게 된다. 따라서 이너셔는 더 작아진다.

- 이너셔 k를 함수로 그려보면

  - ![그림9-8](https://user-images.githubusercontent.com/52944554/160783614-1344b7ab-10cd-44a6-9c04-3b8d7b95a734.PNG)
    - 이너셔 그래프를 클러스터 개수 k의 함수로 그렸을 때 그래프가 꺽이는 지점을 엘보라고 부른다.
  - 이너셔는 k가 4까지 증가할 때 빠르게 줄어든다. 하지만 k가 계속 증가하면 이너셔는 훨씬 느리게 감소한다. 따라서 k에 대한 정답을 모른다면 4는 좋은 선태이 된다.

- 근데 이 방법은 너무 엉성하다.

- 더 정확한 (하지만 계산 비용이 많이 드는) 방법은 **실루엣 점수**이다.

- 이 값은 모든 샘플에 대한 실루엣 계수의 평균이다.

- 샘플의 실루엣 계수 : (b-a) / max(a, b)

  - a : 동일한 클러스터에 있는 다른 샘플까지 평균 거리이다. (즉 클러스터 내부의 평균 거리)
  - b : 가장 가까운 클러스터 까지 평균 거리이다.(즉 가장 가까운 클러스터의 샘플까지 평균 거리. 샘플과 가장 가까운 클러스터는 자신이 속한 클러스터는 제외하고 b가 최소인 클러스터이다.)
  - -1 ~ +1 까지 바뀔 수 있다.
    - +1에 가까우면 자신의 클러스터 안에 잘 속해있고 다른 클러스터와는 멀리 떨어져있다는 의미이다.
    - 0에 가까우면 클러스터 경계에 위치한다는 의미이다.
    - -1에 가까우면 이 샘플이 잘못된 클러스터에 할당되었다는 의미이다.

- 실루엣 점수 계산 silhouette_socre() 함수

- ```python
  from sklearn.metrics import silhouette_score
  silhouette_score(X, kmeans.labels_)
  >>0.655517642572828
  ```

- 클러스터 개수를 달리하여 실루엣 점수를 비교해보면

- ![그림9-9](https://user-images.githubusercontent.com/52944554/160783653-3818f3c7-8a26-46a8-bd21-4f8f81fe79b9.PNG)

  - 실루엣 점수를 사용해 클러스터 개수 k를 선택하기

- 위 그래프를 보면 k=4가 좋은 선택이지만 k=5도 꽤 좋다는 사실을 잘 보여준다. 특히 k=6, 7보다 훨씬 좋다.

- 이는 이너셔를 비교했을 때는 드러나지 않았다.

- 모든 샘플의 실루엣 계수를 할당한 클러스터와 계숫값으로 정렬하여 그리면 더 많은 정보가 있는 그래프를 얻을 수 있다. -> **실루엣 다이어그램**

- 클러스터마다 칼 모양의 그래프가 그려지는데 이 그래프의 높이는 클러스터가 포함하고 있는 샘플의 개수를 의미하고 너비는 이 클러스터에 포함된 샘플의 정렬된 실루엣 계수를 나타낸다(넓을수록 좋다).

- ![그림9-10](https://user-images.githubusercontent.com/52944554/160783676-d9b9589a-b28c-4ef2-878a-448e4c77fcb9.PNG)

  - 여러가지 k값에 대한 실루엣 다이어그램 분석

- 수직 파선은 각 클러스터 개수에 해당하는 실루엣 점수를 나타낸다. 

- 한 클러스터의 샘플 대부분이 이 점수보다 낮은 계수를 가지면(즉 많은 샘플이 파선의 왼쪽에서 멈추면) 클러스터의 샘플이 다른 클러스터랑 너무 가깝다는 것을 의미한다. 따라서 나쁜 클러스터이다.

- k=3, 6에서 나쁜 클러스터를 볼 수 있다. 

- k=4, 5에서는 클러스터가 좋아보인다. 대부분의 샘플이 파선을 넘어서 뻗고 있고 1.0에 근접해 있다. k=4일 때 인덱스 1의 클러스터가 매우 크다. k=5일 때는 모든 클러스터의 크기가 비슷하다. 

- 따라서 k=4일 때 전반적인 실루엣 점수가 k=5보다 조금 높더라도 비슷한 크기의 클러스터를 얻을 수 있는 k=5를 선택하는 것이 좋다.