# CHAPTER 8 차원 축소

- 많은 경우 머신러닝 문제는 훈련 샘플 각각이 수천 수백만 개의 특성을 가지고 있다. 많은 특성은 훈련을 느리게 할 뿐 아니라, 좋은 솔루션을 찾기 어렵게 만든다. 이런 문제를 **차원의 저주(curse of dimensionality)** 라고 한다.
- 실전 문제에서는 특성 수를 크게 줄여서 불가능한 문제를 가능한 범위로 변경할 수 있는 경우가 많다.
  - MNIST 이미지
    - 이미지 경계에 있는 픽셀은 거의 항상 흰색이므로 훈련 세트에서 이런 픽셀을 완전히 제거해도 많은 정보를 잃지 않는다.
- 훈련 속도를 높이는 것 외에 차원 축소는 데이터 시각화(data visualization. Data Viz)에도 아주 유용하다.
- 차원 수를 둘 혹은 셋으로 줄이면 고차원 훈련 세트를 하나의 압축된 그래프로 그릴 수 있고 군집 같은 시각적인 패턴을 감지해 중요한 통찰을 얻는 경우가 많다.
- 데이터 과학자가 아닌 사람들, 특히 최종 결과를 사용하는 결정권자에게 판단을 설명하는데 데이터 시각화는 필수적이다.
- 이 장에서
  - 차원의 저주에 대해 논의한다.
  - 고차원 공간에서 어떤 일이 일어나는지 알아본다.
  - 차원 축소에 사용되는 두 가지 주요 접근 방법(투영, 매니폴드 학습) 소개한다.
  - 가장 인기 있는 차원 축소 기법인 PCA, 커널 PCA, LLE를 다룬다.



## 8.1 차원의 저주

- 대부분의 훈련 데이터가 서로 멀리 떨어져 있다. 이는 새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다는 뜻이다.
- 이 경우 예측을 위해 훨씬 많은 외삽*(extrapolation)을 해야 하기 때문에  저차원일 때보다 예측이 더 불안하다. 간단히 말해 훈련 세트의 차원이 클수록 과대적합 위험이 커진다.
- 이론적으로 차원의 저주를 해결하는 해결책 하나는 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 것입니다.
- 불행하게도 실제로는 일정 밀도에 도달하기 위해 필요한 훈련 샘플 수는 차원 수가 커짐에 따라 기하급수적으로 늘어난다.



*외삽 : 보외법이라고도 하며 수학에서 원래의 관찰 범위를 넘어서 다른 변수와의 관계에 기초하여 변수의 값을 추정하는 과정이다.



## 8.2 차원 축소를 위한 접근 방법

- 투영
- 매니폴드 학습

### 8.2.1 투영

- 대부분의 실전 문제는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져있지 않다. 많은 특성은 거의 변화가 없는 반면, 다른 특성들은 서로 강하게 연관되어 있다.
- 결과적으로 모든 훈련 샘플이 고차원 공간 안에 저차원 부분 공간(subspace)에 (또는 가까이) 놓여있다.
- 예를 들면 
  - ![그림8-2](https://user-images.githubusercontent.com/52944554/160782899-e69f9776-8248-422c-a71a-7bc7bd6265b0.jpg)
  - 원 모양을 띤 3차원 데이터셋이 있다.
  - 모든 훈련 샘플이 거의 평면 형태로 놓여있다.
  - 이것이 고차원(3D) 공간에 있는 저차원(2D) 부분 공간이다.
  - 여기서 모든 훈련 샘플을 이 부분 공간에 수직으로(즉, 샘플과 평면 사이의 가장 짧은 직선을 따라) 투영하면 2D 데이터셋을 얻는다. 
  - ![그림8-3](https://user-images.githubusercontent.com/52944554/160782951-49066313-730c-48f1-b460-47b3b58563cb.jpg)
  - 이럼 데이터텟 차원을 3D -> 2D로 줄였다. 각 축은 (평면에 투영된 좌표인) 새로운 특성 z1 과 z2에 대응된다.
- 차원 축소에서 투영이 언제나 최선의 방법은 아니다. 스위스 롤(Swiss roll) 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있기도 한다.
  - ![그림8-4](https://user-images.githubusercontent.com/52944554/160783000-c0305c82-aab2-4243-8742-6c0459cb8a9c.jpg)
  - 평면에 투영시키면 층이 서로 뭉개진다. 
  - ![그림8-5](https://user-images.githubusercontent.com/52944554/160783040-d62211d6-95d1-44c8-9c59-761edb2d185d.jpg)


### 8.2.2 매니폴드 학습

- 스위스 롤은 2D 매니폴드의 한 예이다.
- 간단히 말해  2D 매니폴드는 고차원 공간에서 휘어지거나 뒤틀린 2D 모양이다.
- 더 일반적으로 d차원 매니폴드는 국부적(전체의 어느 한 부분에만 한정되는)으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부이다(d<n). 스위스 롤의 경우 d = 2, n = 3이다. 국부적으로는 2D 평면으로 보이지만 3차원으로 말려있다.
- 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링하는 식으로 작동한다. 이를 매니폴드 학습이라고 한다.
- 실제 고차원 데이터셋이 더 낳은 저차원 매니폴드에 가깝게 놓여 있다는 매니폴드 가정 또는 매니폴드 가설에 근거한다.
- MNIST 데이터셋으로 생각하면 손글씨 숫자 이미지는 어느정도 비슷한 면이 있다. 숫자 이미지를 만들 때 가능한 자유도는 아무 이미지나 생성할 때의 자유도보다 훨씬 낮다. 이런 제약은 데이터셋을 저차원 매니폴드로 압축할 수 있도록 도와준다.
- 매니폴드 가정은 종종 암묵적으로 다른 가정과 병행되곤 한다. 바로 처리해햐 할 작업(예를 들면 분류나 회귀)이 저차원의 매니폴드 공간에 표현되면 더 간단해질 것이란 가정이다.
- ![그림8-6](https://user-images.githubusercontent.com/52944554/160783088-411686a1-2609-478b-906f-c5263b548803.jpg)
- 이러한 암묵적인 가정이 항상 유효하지는 않다.
- 두 번째 행의 경우 결정 경계가 x1 = 5에 놓여있다. 이 결정 경계는 3D 공간에서는 매우 단순하다(수직 평면). 하지만 펼쳐진 매니폴드에서는 결정 경계가 더 복잡해졌다(네 개의 독립된 수직선).
- 요약하면 모델을 훈련시키기 전에 훈련 세트의 차원을 감소시키면 훈련 속도는 빨라지지만 항상 더 낫거나 간단한 솔루션이 되는 것은 아니다. 이는 전적으로 데이터셋에 달렸다.



